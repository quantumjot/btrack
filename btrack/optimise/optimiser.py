#!/usr/bin/env python
# -------------------------------------------------------------------------------
# Name:     btrack
# Purpose:  A multi object tracking library, specifically used to reconstruct
#           tracks in crowded fields. Here we use a probabilistic network of
#           information to perform the trajectory linking. This method uses
#           positional and visual information for track linking.
#
# Authors:  Alan R. Lowe (arl) a.lowe@ucl.ac.uk
#
# License:  See LICENSE.md
#
# Created:  14/08/2014
# -------------------------------------------------------------------------------

__author__ = "Alan R. Lowe"
__email__ = "a.lowe@ucl.ac.uk"

from btrack.constants import GLPK_OPTIONS, Fates

from . import hypothesis

import logging

import numpy as np
from cvxopt import matrix, spmatrix
from cvxopt.glpk import ilp

# get the logger instance
logger = logging.getLogger(__name__)

INIT_FATES = (
    Fates.INITIALIZE,
    Fates.INITIALIZE_BORDER,
    Fates.INITIALIZE_FRONT,
    Fates.INITIALIZE_LAZY,
)
TERM_FATES = (
    Fates.TERMINATE,
    Fates.TERMINATE_BORDER,
    Fates.TERMINATE_BACK,
    Fates.TERMINATE_LAZY,
)


class TrackOptimiser:
    """The optimizer takes a list of tracklets, as producing by a tracking
    algorithm, such as BayesianTracker, and tries to resolve small linkage
    errors, finding a globally optimal solution.

    Parameters
    ----------
    options : dict
        A dictionary of options to pass to the GLPK optimizer.


    Attributes
    ----------
    hypotheses : list[hypothesis.Hypothesis]
        A list of hypotheses generated by the hypothesis engine.

    Notes
    -----
    General types of linkage error to resolve:
        Track lost at periphery -> Terminate the track
        Track lost briefly, same state -> Link tracklets together by merging
        Object splitting (eg cell division) -> Do not merge, but create graph

    The algorithm proceeds as follows:
        1. Calculate the hypotheses for each tracklet
        2. Solve the association problem/globally optimise
        3. Merge/'link' trajectories
        4. Assign a 'fate' or optimal hypothesis to each trajectory

    The global optimisation utilises integer optimisation from the GLPK library
    Takes in the list of hypotheses and formulates as a mixed integer linear
    programming problem.  Then attempts to use GLPK to solve the association
    problem, returns a list of hypotheses to act upon.

    We set up a constraints matrix, A in this manner: (num_hypotheses x 2N)
    Uses cvxopt.gplk.ilp to:

    minimize    c'*x
    subject to  G*x <= h
                A*x = b
                x[I] are all integer (empty set in this case)
                x[B] are all binary

    Since this is a *minimisation* we need to invert the (log) probability of
    each hypothesis. If the hypotheses are badly formed, this can take *FOREVER*.

    'Report Automated Cell Lineage Construction' Al-Kofahi et al.
    Cell Cycle 2006 vol. 5 (3) pp. 327-335

    'Reliable cell tracking by global data association', Bise et al.
    2011 IEEE Symposium on Biomedical Imaging pp. 1004-1010

    'Local cellular neighbourhood controls proliferation in cell
    competition', Bove A, Gradeci D, Fujita Y, Banerjee S, Charras G and
    Lowe AR 2017 Mol. Biol. Cell vol 28 pp. 3215-3228
    """

    def __init__(self, options: dict = GLPK_OPTIONS):
        self._hypotheses: list[hypothesis.Hypothesis] = []
        self.options = options  # TODO(arl): do some option parsing?

    @property
    def hypotheses(self):
        return self._hypotheses

    @hypotheses.setter
    def hypotheses(self, hypotheses):
        self._hypotheses = hypotheses

    def _build_constraints_matrix_vectorized(  # noqa: PLR0915
        self, n_hypotheses: int, N: int
    ):
        """Build constraints matrix using vectorized operations.

        Parameters
        ----------
        n_hypotheses : int
            Number of hypotheses
        N : int
            Maximum track ID

        Returns
        -------
        A : spmatrix
            Sparse constraints matrix (2N x n_hypotheses)
        rho : matrix
            Vector of hypothesis log-likelihoods
        """

        # anon function to renumber track ID from C++
        def trk_idx(_h):
            return int(_h) - 1

        # Pre-allocate arrays for sparse matrix triplets (row, col, val)
        # Estimate max size: most hypotheses contribute 1-3 entries
        max_entries = n_hypotheses * 3
        rows = np.zeros(max_entries, dtype=np.int32)
        cols = np.zeros(max_entries, dtype=np.int32)
        entry_idx = 0

        # Allocate rho vector
        rho = matrix(0.0, (n_hypotheses, 1), "d")

        # Extract hypothesis data into arrays for vectorized processing
        h_types = np.array(
            [h.hypothesis_type.value for h in self.hypotheses], dtype=np.int32
        )
        h_ids = np.array([trk_idx(h.ID) for h in self.hypotheses], dtype=np.int32)
        h_log_likes = np.array(
            [h.log_likelihood for h in self.hypotheses], dtype=np.float64
        )

        # Set all log likelihoods at once
        rho[:] = h_log_likes.reshape(-1, 1)

        # Process FALSE_POSITIVE hypotheses
        mask = h_types == Fates.FALSE_POSITIVE.value
        if np.any(mask):
            indices = np.where(mask)[0]
            n = len(indices)
            # Each FALSE_POSITIVE adds 2 entries:
            # A[trk, counter] = 1, A[N+trk, counter] = 1
            rows[entry_idx : entry_idx + n] = h_ids[mask]
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n
            rows[entry_idx : entry_idx + n] = N + h_ids[mask]
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n

        # Process INIT_FATES hypotheses
        for fate in INIT_FATES:
            mask = h_types == fate.value
            if np.any(mask):
                indices = np.where(mask)[0]
                n = len(indices)
                # Each INIT adds 1 entry: A[N+trk, counter] = 1
                rows[entry_idx : entry_idx + n] = N + h_ids[mask]
                cols[entry_idx : entry_idx + n] = indices
                entry_idx += n

        # Process TERM_FATES hypotheses
        for fate in TERM_FATES:
            mask = h_types == fate.value
            if np.any(mask):
                indices = np.where(mask)[0]
                n = len(indices)
                # Each TERM adds 1 entry: A[trk, counter] = 1
                rows[entry_idx : entry_idx + n] = h_ids[mask]
                cols[entry_idx : entry_idx + n] = indices
                entry_idx += n

        # Process APOPTOSIS hypotheses
        mask = h_types == Fates.APOPTOSIS.value
        if np.any(mask):
            indices = np.where(mask)[0]
            n = len(indices)
            # Each APOPTOSIS adds 1 entry: A[trk, counter] = 1
            rows[entry_idx : entry_idx + n] = h_ids[mask]
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n

        # Process LINK hypotheses (require additional data)
        mask = h_types == Fates.LINK.value
        if np.any(mask):
            indices = np.where(mask)[0]
            link_ids = np.array(
                [trk_idx(self.hypotheses[i].link_ID) for i in indices], dtype=np.int32
            )
            n = len(indices)
            # Each LINK adds 2 entries: A[trk_i, counter] = 1, A[N+trk_j, counter] = 1
            rows[entry_idx : entry_idx + n] = h_ids[mask]
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n
            rows[entry_idx : entry_idx + n] = N + link_ids
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n

        # Process DIVIDE hypotheses (require additional data)
        mask = h_types == Fates.DIVIDE.value
        if np.any(mask):
            indices = np.where(mask)[0]
            child_one_ids = np.array(
                [trk_idx(self.hypotheses[i].child_one_ID) for i in indices],
                dtype=np.int32,
            )
            child_two_ids = np.array(
                [trk_idx(self.hypotheses[i].child_two_ID) for i in indices],
                dtype=np.int32,
            )
            n = len(indices)
            # Each DIVIDE adds 3 entries
            rows[entry_idx : entry_idx + n] = h_ids[mask]
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n
            rows[entry_idx : entry_idx + n] = N + child_one_ids
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n
            rows[entry_idx : entry_idx + n] = N + child_two_ids
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n

        # Process MERGE hypotheses (require additional data)
        mask = h_types == Fates.MERGE.value
        if np.any(mask):
            indices = np.where(mask)[0]
            parent_one_ids = np.array(
                [trk_idx(self.hypotheses[i].parent_one_ID) for i in indices],
                dtype=np.int32,
            )
            parent_two_ids = np.array(
                [trk_idx(self.hypotheses[i].parent_two_ID) for i in indices],
                dtype=np.int32,
            )
            n = len(indices)
            # Each MERGE adds 3 entries
            rows[entry_idx : entry_idx + n] = N + h_ids[mask]
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n
            rows[entry_idx : entry_idx + n] = parent_one_ids
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n
            rows[entry_idx : entry_idx + n] = parent_two_ids
            cols[entry_idx : entry_idx + n] = indices
            entry_idx += n

        # Trim arrays to actual size and create sparse matrix
        rows = rows[:entry_idx].tolist()
        cols = cols[:entry_idx].tolist()
        vals = [1.0] * entry_idx

        # Create sparse matrix with all entries at once
        A = spmatrix(vals, rows, cols, (2 * N, n_hypotheses), "d")

        return A, rho

    def optimise(self):
        """Run the opimization algorithm.

        Returns
        -------
        optimized : list
            The list of selected hypotheses that forms the optimal solution.
        """

        logger.info("Setting up constraints matrix for global optimisation...")
        if self.options:
            logger.info(f"Using GLPK options: {self.options}...")

        # calculate the number of hypotheses, could use this moment to cull?
        n_hypotheses = len(self.hypotheses)
        N = max(int(h.ID) for h in self.hypotheses)

        # Build constraints matrix using vectorized operations
        A, rho = self._build_constraints_matrix_vectorized(n_hypotheses, N)

        logger.info("Optimizing...")

        # now set up the ILP solver
        G = spmatrix([], [], [], (2 * N, n_hypotheses), "d")
        h = matrix(0.0, (2 * N, 1), "d")  # NOTE h cannot be a sparse matrix
        Ix = set()  # empty set of x which are integer
        B = set(range(n_hypotheses))  # signifies all are binary in x
        b = matrix(1.0, (2 * N, 1), "d")

        # now try to solve it!!!
        status, x = ilp(-rho, -G, h, A, b, Ix, B, options=self.options)

        # log the warning if not optimal solution
        if status != "optimal":
            logger.warning(f"Optimizer returned status: {status}")
            return []

        # return only the selected hypotheses
        results = [i for i, h in enumerate(self.hypotheses) if x[i] > 0]

        logger.info(f"Optimization complete. (Solution: {status})")
        return results
